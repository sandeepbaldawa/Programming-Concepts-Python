Goals(Why monitor)?
====
- Analyzing long-term trends. How big is my database and how fast is it growing? How quickly is my daily-active 
  user count growing?
- Comparing over time or experiment groups
  Are queries faster with Acme Bucket of Bytes 2.72 versus Ajax DB 3.14? 
  How much better is my memcache hit rate with an extra node? 
  Is my site slower than it was last week?  
- Alerting
  Something is broken, and somebody needs to fix it right now! Or, something might break soon, so somebody 
  should look soon.
- Building dashboards
  Dashboards should answer basic questions about your service, 
  and normally include some form of the four golden signals.
- Conducting adhoc retrospective analysis (i.e., debugging)
  Our latency just shot up; what else happened around the same time?  
- System monitoring is also helpful in supplying raw input into business analytics 
- Facilitating analysis of security breaches
- Black-box monitoring
  Testing externally visible behavior as a user would see it.
- White-box monitoring
  Monitoring based on metrics exposed by the internals of the system, including logs,
  interfaces like the Java Virtual Machine Profiling Interface, or an HTTP handler that emits internal statistics.  
- Dashboard
  An application (usually web-based) that provides a summary view of a service’s core metrics. 
  A dashboard may have filters, selectors, and so on, but is prebuilt to expose the metrics most important to its users.  
- Root cause
  A defect in a software or human system that, if repaired, instills confidence that this event won’t happen again 
  in the same way. A given incident might have multiple root causes: for example, perhaps it was caused by a combination
  of insufficient process automation, software that crashed on bogus input, and insufficient testing of the script used 
  to generate the configuration. Each of these factors might stand alone as a root cause, and each should be repaired.
- capacity planning and traffic prediction
  Base on disk filing up or cluster filing up or based on the runnign traffic.
-   
- Good interface, with minimum clicking to get to good data.
- Qualitative measurement of metrics with deterministic outcome.
- Collect network, application and performance from remote sites and analyze
- visualizing operational metrics of various services
- easily configurables metrics(publisher/subscriber)
- calibrating metrics to proper intervals to avoid itter introduced by time-boxing smaller intervals
- Alerting to predict patterns of behavior
- Automation for most tasks like collecting, adding, analyzing a metric.
- Abilty to co-relate different metrics without much user intervention
- Reduce noise by giving capability to serives/users to listen to only what one is interested in
- Easy to add new people and keep up with the growth of new engineers.
- Not much touch points to add a metric. Metrics adding should be automated and error free
- Consistent set of metrics to work with
- Optimum performance(eg:- time it takes for queries) and space usage.
- automated monitoring important because of the sheer scale of services to monitor. Also to reduce noise
  Publisher subscriber would be better applicable. Automated reporting too is equally important.
- Subscribers can be microservices which inrepret this data.  
- Generating graphs/pictures important for understanding behavior/anamolies etc mainly because of the sheer amount of data
  present, it will be very difficult to understand the same manually. 
- Policies and response rules to judge the system

Design
=======
1. Graphing tools provide a clear and nearly-immediate signal that something is broken (and badly). 
We can use this signal to trigger alerts and escalate to someone to take a look. So it's important we
have few entities

Data collection from various sources
  A script running on each host discovers the services running, parses the GC log for that service, and pumps out 
  metrics. 
  Granularity is very important as it allows one to zoom in to collect more details.
  Ability to collect different metrics and from heterogenous sources
  eg:-
  Accessibility of intranet and internet sites
  Collecting metrics like packet loss, latency, jitter between all sites and datacenters with QOS Queues
  Intranet and internet link failover check
  Accessibility of cloud services
  Accessibility of critical corporate applications
  Speed / throughput of network (intranet/internet)

Modules generating graphs from these sources
Services have flexibility to subscribe to which graph to monitor specific characteristics
Policy manager to decide when to generate alerts


Collecting metrics
=================
- Each Service can generate metrics.
- Collect the standard cross-service metrics automatically from every service
- broker which can publish these services and interested parties can subscribe to the same.
- resolution of metrics(intervals they are served) important.
- capacity planning for infrastructure important, since amount of data is huge
   understand WCU, RCU, IOPs, throughput, latency, 
- Configuring(adding/removing) metrics should be easy at various levels, GUI, command line, code etc.
- Programatically name the metrics so we can do things based on the format
- A push based system(forcasting what might be needed) that always sent the metrics it had regardless of being
  in a data store or not

Graphing frontend
================

- RRDtool is the OpenSource industry standard, high performance data logging and graphing system 
for time series data. RRDtool can be easily integrated in shell scripts, perl, python, ruby, lua or tcl applications.
- 



  
