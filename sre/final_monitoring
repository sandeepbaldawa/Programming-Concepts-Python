Goals(Why monitor)?
====
- Analyzing long-term trends. How big is my database and how fast is it growing? How quickly is my daily-active 
  user count growing?
- Comparing over time or experiment groups
  Are queries faster with Acme Bucket of Bytes 2.72 versus Ajax DB 3.14? 
  How much better is my memcache hit rate with an extra node? 
  Is my site slower than it was last week?  
- Alerting
  Something is broken, and somebody needs to fix it right now! Or, something might break soon, so somebody 
  should look soon.
- Building dashboards
  Dashboards should answer basic questions about your service, 
  and normally include some form of the four golden signals.
- Conducting adhoc retrospective analysis (i.e., debugging)
  Our latency just shot up; what else happened around the same time?  
- System monitoring is also helpful in supplying raw input into business analytics 
- Facilitating analysis of security breaches
- Black-box monitoring
  Testing externally visible behavior as a user would see it.
- White-box monitoring
  Monitoring based on metrics exposed by the internals of the system, including logs,
  interfaces like the Java Virtual Machine Profiling Interface, or an HTTP handler that emits internal statistics.  
- Dashboard
  An application (usually web-based) that provides a summary view of a service’s core metrics. 
  A dashboard may have filters, selectors, and so on, but is prebuilt to expose the metrics most important to its users.  
- Root cause
  A defect in a software or human system that, if repaired, instills confidence that this event won’t happen again 
  in the same way. A given incident might have multiple root causes: for example, perhaps it was caused by a combination
  of insufficient process automation, software that crashed on bogus input, and insufficient testing of the script used 
  to generate the configuration. Each of these factors might stand alone as a root cause, and each should be repaired.
- capacity planning and traffic prediction
  Base on disk filing up or cluster filing up or based on the runnign traffic.
- Push
  Any change to a service’s running software or its configuration.
- Understanding symtoms vs cause
  our monitoring system should address two questions: what’s broken, and why?

The "what’s broken" indicates the symptom; the "why" indicates a (possibly intermediate) cause  
- Good interface, with minimum clicking to get to good data.
- Qualitative measurement of metrics with deterministic outcome.
- Collect network, application and performance from remote sites and analyze
- visualizing operational metrics of various services
- easily configurables metrics(publisher/subscriber)
- calibrating metrics to proper intervals to avoid itter introduced by time-boxing smaller intervals
- Alerting to predict patterns of behavior
- Automation for most tasks like collecting, adding, analyzing a metric.
- Abilty to co-relate different metrics without much user intervention
- Reduce noise by giving capability to serives/users to listen to only what one is interested in
- Easy to add new people and keep up with the growth of new engineers.
- Not much touch points to add a metric. Metrics adding should be automated and error free
- Consistent set of metrics to work with
- Optimum performance(eg:- time it takes for queries) and space usage.
- automated monitoring important because of the sheer scale of services to monitor. Also to reduce noise
  Publisher subscriber would be better applicable. Automated reporting too is equally important.
- Subscribers can be microservices which inrepret this data.  
- Generating graphs/pictures important for understanding behavior/anamolies etc mainly because of the sheer amount of data
  present, it will be very difficult to understand the same manually. 
- Policies and response rules to judge the system


What metrics to use for max result and min noise
================================================
The four golden signals of monitoring are latency, traffic, errors, and saturation.
If you can only measure four metrics of your user-facing system, focus on these four.

1. Latency
The time it takes to service a request. It’s important to distinguish between the latency of successful
requests and the latency of failed requests. For example, an HTTP 500 error triggered due to
loss of connection to a database or other critical backend might be served very quickly; 
however, as an HTTP 500 error indicates a failed request, factoring 500s into your overall latency might result 
in misleading calculations. On the other hand, a slow error is even worse than a fast error! 
Therefore, it’s important to track error latency, as opposed to just filtering out errors.

2. Traffic
A measure of how much demand is being placed on your system, measured in a high-level system-specific metric. 
For a web service, this measurement is usually HTTP requests per second, perhaps broken out by the nature of 
the requests (e.g., static versus dynamic content). For an audio streaming system, this measurement might
focus on network I/O rate or concurrent sessions. For a key-value storage system, this measurement might
be transactions and retrievals per second.

3. Errors
The rate of requests that fail, either explicitly (e.g., HTTP 500s), 
implicitly (for example, an HTTP 200 success response, but coupled with the wrong content), or by policy 
(for example, "If you committed to one-second response times, any request over one second is an error"). 
Where protocol response codes are insufficient to express all failure conditions, 
secondary (internal) protocols may be necessary to track partial failure modes. 
Monitoring these cases can be drastically different: catching HTTP 500s at your load balancer can do a decent 
job of catching all completely failed requests, while only end-to-end system tests can detect that you’re serving
the wrong content.

4. Saturation
How "full" your service is. A measure of your system fraction, emphasizing the resources that are most constrained
(e.g., in a memory-constrained system, show memory; in an I/O-constrained system, show I/O).
Note that many systems degrade in performance before they achieve 100% utilization, so having a utilization target
is essential.

Some important points
=====================
1. A healthy monitoring and alerting pipeline is simple and easy to reason about. It focuses primarily on symptoms for paging,
reserving cause-oriented heuristics to serve as aids to debugging problems. 
2. Monitoring symptoms is easier the further
"up" your stack you monitor, though monitoring saturation and performance of subsystems such as databases often must be
performed directly on the subsystem itself.
3. Email alerts are of very limited value and tend to easily become overrun 
with noise; instead, you should favor a dashboard that monitors all ongoing subcritical problems for the sort of 
information that typically ends up in email alerts. A dashboard might also be paired with a log, in order to analyze
historical correlations.

Over the long haul, achieving a successful on-call rotation and product includes choosing to alert on symptoms or 
imminent real problems, adapting your targets to goals that are actually achievable, and making sure that your monitoring
supports rapid diagnosis.

Design
=======
1. Graphing tools provide a clear and nearly-immediate signal that something is broken (and badly). 
We can use this signal to trigger alerts and escalate to someone to take a look. So it's important we
have few entities

Data collection from various sources
  A script running on each host discovers the services running, parses the GC log for that service, and pumps out 
  metrics. 
  Granularity is very important as it allows one to zoom in to collect more details.
  Ability to collect different metrics and from heterogenous sources
  eg:-
  Accessibility of intranet and internet sites
  Collecting metrics like packet loss, latency, jitter between all sites and datacenters with QOS Queues
  Intranet and internet link failover check
  Accessibility of cloud services
  Accessibility of critical corporate applications
  Speed / throughput of network (intranet/internet)

Modules generating graphs from these sources
Services have flexibility to subscribe to which graph to monitor specific characteristics
Policy manager to decide when to generate alerts


Collecting metrics
=================
- Each Service can generate metrics.
- Collect the standard cross-service metrics automatically from every service
- broker which can publish these services and interested parties can subscribe to the same.
- resolution of metrics(intervals they are served) important.
- capacity planning for infrastructure important, since amount of data is huge
   understand WCU, RCU, IOPs, throughput, latency, 
- Configuring(adding/removing) metrics should be easy at various levels, GUI, command line, code etc.
- Programatically name the metrics so we can do things based on the format
- A push based system(forcasting what might be needed) that always sent the metrics it had regardless of being
  in a data store or not

Graphing frontend
================

- RRDtool is the OpenSource industry standard, high performance data logging and graphing system 
for time series data. RRDtool can be easily integrated in shell scripts, perl, python, ruby, lua or tcl applications.
- 



  
