https://www.scalyr.com/community/guides/zen-and-the-art-of-system-monitoring
https://landing.google.com/sre/book/chapters/monitoring-distributed-systems.html

Why Monitor?
============

There are many reasons to monitor a system, including:

1. Analyzing long-term trends
How big is my database and how fast is it growing? How quickly is my daily-active user count growing?

2. Comparing over time or experiment groups
Are queries faster with Acme Bucket of Bytes 2.72 versus Ajax DB 3.14? How much better is my memcache hit rate with an extra node? Is my site slower than it was last week?

3. Alerting
Something is broken, and somebody needs to fix it right now! Or, something might break soon, so somebody should look soon.

4. Building dashboards
Dashboards should answer basic questions about your service, and normally include some form of the four golden signals (discussed later in this chapter).

5. Conducting adhoc retrospective analysis (i.e., debugging)
Our latency just shot up; what else happened around the same time?

6. System monitoring is also helpful in supplying raw input into business analytics 

7. Facilitating analysis of security breaches

Lots of white-box monitoring
Some black-box monitoring for critical stuff
Four golden signals
Latency
Traffic
Errors
Saturation

Black-box monitoring
====================
Testing externally visible behavior as a user would see it.

White-box monitoring
====================
Monitoring based on metrics exposed by the internals of the system, including logs,
interfaces like the Java Virtual Machine Profiling Interface, or an HTTP handler that emits internal statistics.

Dashboard
=========
An application (usually web-based) that provides a summary view of a service’s core metrics. 
A dashboard may have filters, selectors, and so on, but is prebuilt to expose the metrics most important to its users.

Root cause
==========
A defect in a software or human system that, if repaired, instills confidence that this event won’t happen again in the same way. A given incident might have multiple root causes: for example, perhaps it was caused by a combination of insufficient process automation, software that crashed on bogus input, and insufficient testing of the script used to generate the configuration. Each of these factors might stand alone as a root cause, and each should be repaired.


Alert
======
A notification intended to be read by a human and that is pushed to a system such as a bug or ticket queue, 
an email alias, or a pager. Respectively, these alerts are classified as tickets, email alerts,1 and pages.
The dashboard might also display team information such as ticket queue length, a list of high-priority bugs, 
the current on-call engineer for a given area of responsibility, or recent pushes.

Outages can be prolonged because
other noise interferes with a rapid diagnosis and fix. Effective alerting systems have good signal and very low noise.
When pages occur too frequently, employees second-guess, skim, or even ignore incoming alerts, 
sometimes even ignoring a "real" page that’s masked by the noise.
Very important to balance between right amount of alerting, info, warning etc. types of messages to avoid too much information
which is difficult to traige.

Node and machine
================
Used interchangeably to indicate a single instance of a running kernel in either a physical server, virtual machine, or container. There might be multiple services worth monitoring on a single machine. The services may either be:

Related to each other: for example, a caching server and a web server
Unrelated services sharing hardware: for example, a code repository and a master for a configuration system like Puppet or Chef

Push
Any change to a service’s running software or its configuration.

capacity planning and traffic prediction
========================================
Base on disk filing up or cluster filing up or based on the runnign traffic..

Policies and response rules
===========================

Minotiring capacity and plan capacity
Monitor failure rate of devices, nodes, h/w etc..and predict when we would need a new device and alert user..
try to make alerting as specific as possible eg:- db is slow instead of website is slow..
asup emails..

Understanding symtoms vs cause
===============================
our monitoring system should address two questions: what’s broken, and why?

The "what’s broken" indicates the symptom; the "why" indicates a (possibly intermediate) cause


Symtoms => Cause
=================
I’m serving HTTP 500s or 404s => Database servers are refusing connections 
My responses are slow => CPUs are overloaded by a bogosort, or an Ethernet cable is crimped under a rack,
                         visible as partial packet loss
Users in Antarctica aren’t receiving animated cat GIFs => Your Content Distribution Network hates scientists and felines, 
                                                          and thus blacklisted some client IPs
Private content is world-readable => A new software push caused ACLs to be forgotten and allowed all requests

What metrics to use for max result and min noise
================================================
The four golden signals of monitoring are latency, traffic, errors, and saturation.
If you can only measure four metrics of your user-facing system, focus on these four.

1. Latency
The time it takes to service a request. It’s important to distinguish between the latency of successful
requests and the latency of failed requests. For example, an HTTP 500 error triggered due to
loss of connection to a database or other critical backend might be served very quickly; 
however, as an HTTP 500 error indicates a failed request, factoring 500s into your overall latency might result 
in misleading calculations. On the other hand, a slow error is even worse than a fast error! 
Therefore, it’s important to track error latency, as opposed to just filtering out errors.

2. Traffic
A measure of how much demand is being placed on your system, measured in a high-level system-specific metric. 
For a web service, this measurement is usually HTTP requests per second, perhaps broken out by the nature of 
the requests (e.g., static versus dynamic content). For an audio streaming system, this measurement might
focus on network I/O rate or concurrent sessions. For a key-value storage system, this measurement might
be transactions and retrievals per second.

3. Errors
The rate of requests that fail, either explicitly (e.g., HTTP 500s), 
implicitly (for example, an HTTP 200 success response, but coupled with the wrong content), or by policy 
(for example, "If you committed to one-second response times, any request over one second is an error"). 
Where protocol response codes are insufficient to express all failure conditions, 
secondary (internal) protocols may be necessary to track partial failure modes. 
Monitoring these cases can be drastically different: catching HTTP 500s at your load balancer can do a decent 
job of catching all completely failed requests, while only end-to-end system tests can detect that you’re serving
the wrong content.

4. Saturation
How "full" your service is. A measure of your system fraction, emphasizing the resources that are most constrained
(e.g., in a memory-constrained system, show memory; in an I/O-constrained system, show I/O).
Note that many systems degrade in performance before they achieve 100% utilization, so having a utilization target
is essential.

In complex systems, saturation can be supplemented with higher-level load measurement: can your service properly handle double the traffic, 
handle only 10% more traffic, or handle even less traffic than it currently receives? 
For very simple services that have no parameters that alter the complexity of the request 
(e.g., "Give me a nonce" or "I need a globally unique monotonic integer") that rarely change configuration,
a static value from a load test might be adequate. As discussed in the previous paragraph, however, 
most services need to use indirect signals like CPU utilization or network bandwidth that have a known upper bound. 
Latency increases are often a leading indicator of saturation. Measuring your 99th percentile response time over some
small window (e.g., one minute) can give a very early signal of saturation.

Finally, saturation is also concerned with predictions of impending saturation,
such as "It looks like your database will fill its hard drive in 4 hours."

Some important points
=====================
1. A healthy monitoring and alerting pipeline is simple and easy to reason about. It focuses primarily on symptoms for paging,
reserving cause-oriented heuristics to serve as aids to debugging problems. 
2. Monitoring symptoms is easier the further
"up" your stack you monitor, though monitoring saturation and performance of subsystems such as databases often must be
performed directly on the subsystem itself.
3. Email alerts are of very limited value and tend to easily become overrun 
with noise; instead, you should favor a dashboard that monitors all ongoing subcritical problems for the sort of 
information that typically ends up in email alerts. A dashboard might also be paired with a log, in order to analyze
historical correlations.

Over the long haul, achieving a successful on-call rotation and product includes choosing to alert on symptoms or 
imminent real problems, adapting your targets to goals that are actually achievable, and making sure that your monitoring
supports rapid diagnosis.
