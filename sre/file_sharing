http://blog.marc-seeger.de/2008/07/23/the-current-state-of-anonymous-file-sharing/
http://krondo.com/in-which-we-begin-at-the-beginning/
http://www.hpcs.cs.tsukuba.ac.jp/~tatebe/lecture/h23/dsys/dsd-tutorial.html
http://people.csail.mit.edu/moitra/docs/6854lec3.pdf
https://www.akamai.com/us/en/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf

goal
====
1. The “hot spot problem” is to satisfy all browser page requests while ensuring that with high 
probability no cache or server is swamped.
2. minimize cache memory requirements.
3. minimize the delay a client experiences in obtaining a page.


- content of files changing?

What are hot spots
===================
Hot spots occur any time a large number of clientswish to simultaneously access data from a single server. 
If the site is not provisioned to deal with all of these clients simultaneously,service may be degraded or lost 
eg:- A Web site can suddenly become extremely popular and receive far more requests in a relatively short time.

why consistent hashing
=======================
When we have hot spots requests can clog the server, we can solve this with a proxy and set of caches which serve the requests.
To search which caches client should requests from 
1. any request could go through the proxy to a random cache,
    where we could use multicast betwen the caches to decide where to route the requests. 
    If not found in caches it is fetched from the home server
    However multicast is very inefficient because of number of messages.(**Maplpani)
2. Harvest Cache uses a technique where in leaf nodes are caches assigned to clients making requests and if not found in leaf cache
   the requests propogates to parent and eventually  the root, upon which page is fetched from the home server. Issue happens
   if each request is distinct in which root node will have to fetch pages from home server, this can swap the root node.
3. Plaxton and Rajaraman show how to balance the loadamong all caches by using randomization and hashing.
   Issue are since their algorithm sends a copy of each page request to a random element in every set, the small sets for a 
   popular page are guaranteed to be swamped. Different tree for each page.

Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web (PDF Download Available). Available from: https://www.researchgate.net/publication/2395793_Consistent_Hashing_and_Random_Trees_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web [accessed Aug 31, 2017].
   


Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web (PDF Download Available). Available from: https://www.researchgate.net/publication/2395793_Consistent_Hashing_and_Random_Trees_Distributed_Caching_Protocols_for_Relieving_Hot_Spots_on_the_World_Wide_Web [accessed Aug 31, 2017].


questions:-
==========
How often do we do the transfer?  Is it a one-time or irregular transfer or is it a routine syncronization?
How remote is the target?  Is it an in-office network,
is it using an in-house remote network, or is it a remote copy across the "public" Internet?


machines:- sizing, heterogenous, IOPs, queueing, upload/download capacity, 

server:- async, sync?, vertical scaling to handle threads synchronously? or have asynch threading model
         load balancing in place?

Network:- bandwidth?, network localization, i.e how many hops are different machines.
latency, link quality, reliability on the network size
TCP windowing/sequencing
NAT-ing, and NAT Traversal 

data:- 
1. data pattern?, 

2. how to verify integrity (md5 hash)

3. can the file be archived and send, 

4. IO size per day

5. how much of the data set do they access

6. is there any correlation between io pattern access between two clients

7. do we have to handle the long tail access pattern

Design:-
Either divide into chunks or batch these files

OR

So First select a sample of 100 nodes and do a speedtest across these node. 
A speed test will give an indication of what is the median speed which is available across these 100 nodes and may be that acts as a sample to the entire network.
So now you have a value of X Mbps is the speed at which you can do a transfer across to these nodes.

Look at the capcity of your own outgoing data speed. So if the central server has a capacity of YGbps as its upload speed
then the batching size = Your Upload Capacity (Y)/ X(speed found by speedtest).

According to this batching size you move ahead in transferring parallelly to 2000 server in batches.

So I'd probably chunk the file (~32k each), compress each chunk, encrypt each compressed chunk by a key unique to that chunk (derived from the original chunk's hash),
And I'd encrypt my chunk map by a master key and upload that too

Compared to the synchronous model, the asynchronous model performs best when
=============================================================================
1. There are a large number of tasks so there is likely always at least one task that can make progress.
2. The tasks perform lots of I/O, causing a synchronous program to waste lots of time blocking when other tasks could be running.
3. The tasks are largely independent from one another so there is little need for inter-task communication 
(and thus for one task to wait upon another).
4. The I/O part of threaded code is relatively easy but managing the shared state between threads (using locks/queues/etc)
   without race conditions is what makes it tricky. Using an async model means you have less going on at the same time so races are easily avoided. 2/3.
   each thread will consume at least one memory page of stack (4KB or 8KB typically), plus some unknown amount of memory for 
   other data structures related to that thread's state.
5. It is very difficult to write code that is thread safe. 
   With asyncronous code you know exactly
   where the code will shift from one task to the next and race conditions are much therefore much harder to come by. Threads
   consume a fair amount of data since each thread needs to have its own stack;
   with async code all the code shares the same stack and the stack is kept small due to continuously unwinding the stack between tasks. Threads are OS structures
   and are therefore more memory for the platform to support. There is no such problem with asynchronous tasks.


A distributed system is an application that executes a collection of protocols to coordinate the
actions of multiple processes on a network, such that all components cooperate together to perform
a single or small set of related tasks.

To be truly reliable, a distributed system must have the following characteristics:
1. Fault-Tolerant: It can recover from component failures without performing incorrect actions.
2. Highly Available: It can restore operations, permitting it to resume providing services even when some components have failed.
3. Recoverable: Failed components can restart themselves and rejoin the system, after the cause of failure has been repaired.
4. Consistent: The system can coordinate actions by multiple components often in the presence of concurrency and failure.
   This underlies the ability of a distributed system to act like a non-distributed system.
5. Scalable: It can operate correctly even as some aspect of the system is scaled to a larger size. For example, we might increase the size of the network on which the system is running. This increases the frequency of network outages and could degrade a "non-scalable" system. Similarly, we might increase the number of users or servers, or overall load on the system. In a scalable system, this should not have a significant effect.
6. Predictable Performance: The ability to provide desired responsiveness in a timely manner.
7. Secure: The system authenticates access to data and services

Handling failures is an important theme in distributed systems design. Failures fall into two obvious categories: 
hardware and software. Today, problems are most often associated with connections and mechanical devices, 
i.e., network failures, node failures and drive failures.

Heisenbug: A bug that seems to disappear or alter its characteristics when it is observed or researched. 
A common example is a bug that occurs in a release-mode compile of a program, but not when researched under debug-mode. 
The name "heisenbug" is a pun on the "Heisenberg uncertainty principle," a quantum physics term which is commonly (yet inaccurately) used to refer to the way in which observers affect the measurements of the things that they are observing, by the act of observing alone (this is actually the observer effect, and is commonly confused with the Heisenberg uncertainty principle).

Bohrbug: A bug (named after the Bohr atom model) that, in contrast to a heisenbug, does not disappear or 
alter its characteristics when it is researched. A Bohrbug typically manifests itself reliably under a well-defined
set of conditions.
Heisenbugs tend to be more prevalent in distributed systems than in local systems.
One reason for this is the difficulty programmers have in obtaining a coherent and comprehensive view 
of the interactions of concurrent processes.

Let's get a little more specific about the types of failures that can occur in a distributed system:
1. Halting failures: A component simply stops. There is no way to detect the failure except by timeout: it either stops sending "I'm alive" (heartbeat) messages or fails to respond to requests. Your computer freezing is a halting failure.
2. Fail-stop: A halting failure with some kind of notification to other components. A network file server telling its clients it is about to go down is a fail-stop.
3. Omission failures: Failure to send/receive messages primarily due to lack of buffering space, which causes a message to be discarded with no notification to either the sender or receiver. This can happen when routers become overloaded.
4. Network failures: A network link breaks.
5. Network partition failure: A network fragments into two or more disjoint sub-networks within which messages can be sent, but between which messages are lost. This can occur due to a network failure.
6. Timing failures: A temporal property of the system is violated. For example, clocks on different computers which are used to coordinate processes are not synchronized; when a message is delayed longer than a threshold period, etc.
7. Byzantine failures: This captures several types of faulty behaviors including data corruption or loss, failures caused by malicious programs, etc. [1]

when they first build a distributed system, makes the following eight assumptions
The network is reliable.
Latency is zero.
Bandwidth is infinite.
The network is secure.
Topology doesn't change.
There is one administrator.
Transport cost is zero.
The network is homogeneous.    [3]
Latency: the time between initiating a request for data and the beginning of the actual data transfer.
Bandwidth: A measure of the capacity of a communications channel. The higher a channel's bandwidth, the more information it can carry.
Topology: The different configurations that can be adopted in building networks, such as a ring, bus, star or meshed.
Homogeneous network: A network running a single network protocol.

