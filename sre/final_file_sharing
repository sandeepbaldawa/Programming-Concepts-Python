questions
=========
machines:- sizing, heterogenous, IOPs, queueing, upload/download capacity..
Network:- bandwidth, network localization, i.e how many hops are different machines.
latency, link quality, reliability on the network size
- content of files changing? IO patterns


Design
======
1. We have 10K buckets(destination server nodes) in which we need to send the 1TB size file. Hence we could
   take the 1TB file and divide it into 100K chunks such that each bucket would receive 10 chunks. If a machine is more
   powerful CPU/Storage it can get to store more chunks(more slices in the consistent hashing method)
2. We could do the above using consistent hashing. Advantages of distributed hashing with consistent hashing
    a. Node add or remove only K/N keys need to be moved.
    b. we can rebalance as and when needed based on the hots spots.
    c. performance consistency
3. We could first hash 
   a. each machine by maybe it's ip address 
   b. each chunk by the inode + offset
4. We could create replicated copied of chunks. Advantages are
   a. (2-way, 3-way depending on the requirement), comprise storage vs reliability. 
   b. For hot spots reads can be distributed to the replicated copies to spread the load, something like rebalance
      can be done periodically 
5. Some framework like zookeeper which can maintain for a given inode what are the corresponding chunks and 
   they are on which machines. Advantages of zookeeper is
      a. It can maintain an ensemble(master slave) and heartbeat a node to check if some node is dead
      b. Help in easily querying for a file using the inode and check on which machines the corresponding chunks lie.
      c. Can help initiate rebalance. 
6. We could build a persistent caching layer which will allow us to acknowledge writes back quicker.
   what do we optimize on performance or reliability?, we can optimize this by acknowledging by either writing/not writing to
   the mirrors in persistence layers with SSD's, or acknowledging in memory..fundamentally a tradeoff.
7. To save space we could compress the data or dedup it..   
